#!/bin/bash
# shellcheck disable=SC1091
source ember_bash_setup &> /dev/null

trap 'die "A fatal error was reported on ${BASH_SOURCE[0]} line ${LINENO}."' ERR

hostName="$1"
shift
collection="$1"
shift
identifier="$1"
shift
remoteFileName="$1"
shift
fileSizeEstimate="$1"
shift
title="$1"
shift
description="$1"
shift
keywords="$1"
shift
AWS_ACCESS_KEY_ID="$1"
if [[ -z "$AWS_ACCESS_KEY_ID" ]]; then
    AWS_ACCESS_KEY_ID="$(grep access ~/.config/ia.ini | head -n 1 | awk '{ print $3 }')"
fi
shift
AWS_SECRET_ACCESS_KEY="$1"
if [[ -z "$AWS_SECRET_ACCESS_KEY" ]]; then
    AWS_SECRET_ACCESS_KEY="$(grep secret ~/.config/ia.ini | head -n 1 | awk '{ print $3 }')"
fi

WorkDirectory="$(crystallize-getconf WorkDirectory)"

chunkLocationName="$WorkDirectory/$(date +%Y-%m-%d-%H-%M-%S-%N)-$(xxd -pu <<< "$(date +%z)")-$(python -c 'import uuid; print str(uuid.uuid4())').streamingUploadChunk"

#FIXME: only did the first 15 bytes... weird.
dd bs=5242880 count=1 of="$chunkLocationName"

thisChunkSize="$(disk-size-in-bytes "$chunkLocationName")"
if [[ "$thisChunkSize" -lt 5242880 ]]; then
    #Input is less than 5mb; upload normally
    curl --location --raw -X PUT --data-binary @"$chunkLocationName" --header "x-amz-auto-make-bucket:1" --header "x-archive-queue-derive:0" --header "x-archive-size-hint:$fileSizeEstimate" --header "authorization: LOW $AWS_ACCESS_KEY_ID:$AWS_SECRET_ACCESS_KEY" --header "x-archive-meta-collection:$collection" --header "x-archive-meta-title:$title" --header "x-archive-meta-description:$description" --header "x-archive-meta-subject:$keywords" "http://$hostName/$identifier/$remoteFileName"
else
    #Choose a chunk size to use
    #Cannot have more than 10,000 chunks, but fileSizeEstimate may be slightly smaller than the actual size.
    chunkSize=$((fileSizeEstimate / 9000))
    #Cannot have chunk size less than 5MB.
    if [[ $chunkSize -lt 5242880 ]]; then
        chunkSize=5242880
    fi

    #Make bucket
    echo -n "a" | curl --location --raw -X PUT --data-binary @- --header "x-amz-auto-make-bucket:1" --header "x-archive-queue-derive:0" --header "x-archive-size-hint:$fileSizeEstimate" --header "authorization: LOW $AWS_ACCESS_KEY_ID:$AWS_SECRET_ACCESS_KEY" --header "x-archive-meta-collection:$collection" --header "x-archive-meta-title:$title" --header "x-archive-meta-description:$description" --header "x-archive-meta-subject:$keywords" "http://$hostName/$identifier/.s3-streaming-upload.placeholder"

    #Initialize streaming upload
    uploadId="$(aws --endpoint-url "$hostName" --output text --query "UploadId" s3api create-multipart-upload --bucket "$identifier" --key "$remoteFileName")"

    partNumber=1
    printf "{\n  \"Parts\": [" > "$chunkLocationName.json"
    while dd bs=5242880 count=1 of="$chunkLocationName"; test -s "$chunkLocationName"; do
        thisChunkSize="$(disk-size-in-bytes "$chunkLocationName")"
        uploadEtag="$(aws --endpoint-url "$hostName" --output text --query "ETag" s3api upload-part --bucket "$identifier" --key "$remoteFileName" --part-number "$partNumber" --upload-id "$uploadId" --body "$chunkLocationName")"
        printf "    {\n      \"ETag\": \"%s\",\n      \"PartNumber\": %s\n    },\n" "$uploadEtag" "$partNumber" >> "$chunkLocationName.json"
        partNumber=$((partNumber + 1))
        rm "$chunkLocationName"
    done
    #Remove last comma and newline, and finish the file
    truncate -s-2 "$chunkLocationName.json"
    printf "\n  ]\n}" >> "$chunkLocationName.json"

    (
        cd "$(dirname "$chunkLocationName")" || die "Could not cd to directory of $chunkLocationName !"
        aws --endpoint-url "$hostName" s3api complete-multipart-upload --multipart-upload file://"$(basename "$chunkLocationName.json")" --bucket "$identifier" --key "$remoteFileName" --upload-id "$uploadId"
    )
fi
