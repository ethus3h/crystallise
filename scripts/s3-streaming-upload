#!/bin/bash

set -e

crystallizeAccessKey="$(grep access ~/.config/ia.ini | head -n 1 | awk '{ print $3 }')"
crystallizeSecretKey="$(grep secret ~/.config/ia.ini | head -n 1 | awk '{ print $3 }')"

hostName="$1"
shift
collection="$1"
shift
identifier="$1"
shift
remoteFileName="$1"
shift
fileSizeEstimate="$1"
shift
title="$1"
shift
description="$1"
shift
keywords="$1"

WorkDirectory="$(crystallize-getconf WorkDirectory)"

chunkLocationName="$WorkDirectory/$(date +%Y-%m-%d-%H-%M-%S-%N)-$(xxd -pu <<< "$(date +%z)")-$(python -c 'import uuid; print str(uuid.uuid4())').streamingUploadChunk"

Slurp up to first 5mb into file. 

thisChunkSize="$(du "$chunkLocationName")"
if [ "$thisChunkSize" -lt 5242880 ]; then
    #Input is less than 5mb; upload normally
    curl --location --raw -X PUT --data-binary @"$chunkLocationName" --header "x-amz-auto-make-bucket:1" --header "x-archive-queue-derive:0" --header "x-archive-size-hint:$fileSizeEstimate" --header "authorization: LOW $crystallizeAccessKey:$crystallizeSecretKey" --header "x-archive-meta-collection:$collection" --header "x-archive-meta-title:$title" --header "x-archive-meta-description:$description" --header "x-archive-meta-subject:$keywords" "http://$hostName/$identifier/$remoteFileName"
else
    #Cannot have more than 10,000 chunks.
    chunkSize=$((fileSizeEstimate / 10000))
    #Cannot have chunk size less than 5MB.
    if [ $chunkSize -lt 5242880 ]; then
        chunkSize=5242880
    fi

    #Make bucket
    echo -n "a" | curl --location --raw -X PUT --data-binary @- --header "x-amz-auto-make-bucket:1" --header "x-archive-queue-derive:0" --header "x-archive-size-hint:$fileSizeEstimate" --header "authorization: LOW $crystallizeAccessKey:$crystallizeSecretKey" --header "x-archive-meta-collection:$collection" --header "x-archive-meta-title:$title" --header "x-archive-meta-description:$description" --header "x-archive-meta-subject:$keywords" "http://$hostName/$identifier/.s3-streaming-upload.placeholder"

    #Initialize streaming upload
    uploadId="$(aws --endpoint-url "$hostName" --output text --query "UploadId" s3api create-multipart-upload --bucket "$identifier" --key "$remoteFileName")"

    partNumber=1
    printf "{\n  \"Parts\": [" > "$chunkLocationName.json"
    #TODO
    until all stdin is used up; do
        #TODO slurp up to chunkSize into $chunkLocationName
        thisChunkSize="$(du "$chunkLocationName")"
        uploadEtag="$(aws --endpoint-url "$hostName" --output text --query "ETag" s3api upload-part --bucket "$identifier" --key "$remoteFileName" --part-number "$partNumber" --upload-id "$uploadId" --body "$chunkLocationName")"
        partNumber=$((partNumber + 1))
        printf "    {\n      \"ETag\": \"%s\",\n      \"PartNumber\": %s\n    },\n" "$uploadEtag" "$partNumber" >> "$chunkLocationName.json"
    done
    #Remove last comma and newline, and finish the file
    truncate -s-2 "$chunkLocationName.json"
    printf "\n  ]\n}" >> "$chunkLocationName.json"

    (
        cd "$(dirname "$chunkLocationName")"
        aws --endpoint-url "$hostName" s3api complete-multipart-upload --multipart-upload file://"$(basename "$chunkLocationName.json")" --bucket "$identifier" --key "$remoteFileName" --upload-id "$uploadId"
    )
fi
