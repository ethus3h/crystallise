#!/usr/bin/env bash
# shellcheck disable=SC1091
source ember_bash_setup &> /dev/null
#set -x

((EUID)) && exec sudo -- "$0" "$@"
trap 'die "A fatal error was reported on ${BASH_SOURCE[0]} line ${LINENO} in $(pwd) at $(emdate)."' ERR

dbDir="$1"
requestUrl="$2"
if [[ -z "$requestUrl" ]]; then
    dbDir="$(crystallize-getconf WorkDirectory)/.CrystalCache"
    requestUrl="$dbDir"
fi
hashFull="$(sha512sum <<< "$requestUrl" | awk '{print $1;}')"
hashA="${hashFull:0:1}"
hashB="${hashFull:1:1}"
hashC="${hashFull:2:1}"
hashSubdir="$hashA/$hashB/$hashC"
hashDir="$dbDir/$hashSubdir"
mkdir -p "$hashDir"
dbLinkPath="$hashDir/$hashFull"

finish() {
    rm -f "${dbLinkPath:?}.locked"
}
trap finish EXIT

if [[ "$3" != "--lock-override" ]]; then
    if [[ -e "$dbLinkPath.locked" ]]; then
        warn "This URL cache entry is already in use. If the process using it ($(<"$dbLinkPath.locked")) has crashed, remove $dbLinkPath.locked manually."
        warn "Waiting 30 minutes for process using it ($(<"$dbLinkPath.locked")) to finish..."

        waitForLockTries=0
        until [[ "$waitForLockTries" -ge 360 ]]; do
            if [[ -e "$dbLinkPath.locked" ]]; then
                warn "Still waiting, $((waitForLockTries - 360)) tries remaining... If the process using this URL cache entry ($(<"$dbLinkPath.locked")) has crashed, remove $dbLinkPath.locked manually."
            else
                break
            fi
            waitForLockTries=$((waitForLockTries+1))
            sleep 5
        done

        if [[ -e "$dbLinkPath.locked" ]]; then
            die "This URL cache entry has been in use for 30 minutes so giving up. If the process using it ($(<"$dbLinkPath.locked")) has crashed, remove $dbLinkPath.locked manually."
        fi
    fi
fi

checksumName="${dbLinkPath:?}.checksum"
csAvail="false"
[[ -e "$checksumName" ]] && csAvail="true"
tempResolvedFileName="$(readlink "$dbLinkPath")" || true # It's totally ok if this fails.
if [[ ! -h "$dbLinkPath" ]] || [[ ! -s "$tempResolvedFileName" ]] || [[ "$csAvail" == "false" ]]; then
    # Only lock the cache entry while we're editing it
    echo "locked on ${BASH_SOURCE[0]} line ${LINENO} in $(pwd) at $(emdate)" > "$dbLinkPath.locked"

    if [[ -f "$tempResolvedFileName" ]]; then
        rm "$tempResolvedFileName"
    fi
    # Item is not in the cache, so add it
    if ! [[ -f "$dbLinkPath" ]]; then
        # Never seen this item before, so set the requestCount
        requestCount=1
    else
        requestCount="$(<"$dbLinkPath")"
        rm "$dbLinkPath"
    fi
    if [[ -z "$requestCount" ]]; then
        requestCount=1
    fi
    dbFileDir="$dbDir/ByCount/${requestCount:?}/$hashSubdir"
    dbFilePath="$dbFileDir/$hashFull"
    mkdir -p "$dbFileDir"
    if [[ -e "$dbFilePath" ]]; then
        warn "$dbFilePath already seems to exist. Huh? Removing it."
        rm "$dbFilePath"
    fi
    iaIdentifier="$(iaident "$requestUrl")"
    iaPath="$(iapath "$requestUrl")"
    if isIaName "$requestUrl" && [[ "$iaPath" != "$iaIdentifier/$iaIdentifier""_files.xml" ]]; then
        ia downloadAsStream "$requestUrl" > "$dbFilePath"
        retryUntilSuccess 5000 60 5 5 iasha1 "$iaIdentifier/$iaPath" > /dev/null || die "Number of allowed retries exceeded without success."
        [[ "$(sha1sum "$dbFilePath" | awk '{print $1;}')" == "$(retryUntilSuccess 5000 60 5 5 iasha1 "$iaIdentifier/$iaPath")" ]] || die "Checksum known by Internet Archive did not match the retrieved file! Was it corrupted in transit?"
    else
        retryUntilSuccess 100 30 5 5 wget -t 150 -qO "$dbFilePath" "$requestUrl"
    fi
    sha512sum < "$dbFilePath" | awk '{print $1;}' > "$checksumName"
    printf "%s" "$requestUrl" > "$dbLinkPath.url"
    if [[ -e "$dbLinkPath" ]] || [[ -h "$dbLinkPath" ]]; then
        rm "$dbLinkPath"
    fi
    ln -s "$dbFilePath" "$dbLinkPath"

    # Don't release the lock here, so the cache entry we just downloaded doesn't get dropped by scache_gc even if it normally would be
fi
dbFilePath="$(readlink -e "$dbLinkPath")"
# Check that the data were retrieved successfully
[[ -e "$checksumName" ]] || die "Checksum file for requested item is missing!"
knownChecksum="$(<"$checksumName")"
itemChecksum="$(sha512sum < "$dbLinkPath" | awk '{print $1;}')"
if [[ "$itemChecksum" != "$knownChecksum" ]]; then
    rm "$dbFilePath"
    die "File did not match what its checksum should be!"
fi
# First, make sure the link is good and fix it up if it's not.
if ! [[ -f "$dbFilePath" ]]; then
    find "$dbDir/ByCount" -mindepth 4 -maxdepth 4 -type d -name "$hashFull" -exec ln -sf {} "$dbLinkPath" \;
    dbFilePath="$(readlink -e "$dbLinkPath")"
fi
if ! [[ -f "$dbFilePath" ]]; then
    die "Could not find the cache file, despite there being a link to it!"
fi
dbFileOldPath="$dbFilePath"
dbFileDir="$(dirname "$dbFilePath")"
# Get the requestCount. There are three path components after it. I suspect there's a better way to do this.
oldCount="$(basename "$(dirname "$(dirname "$(dirname "$dbFileDir")")")")"
if ! [[ "$oldCount" =~ ^-?[0-9]+$ ]]; then
    rm "$dbLinkPath"
    oldCount="1"
fi
# Increment the item's requestCount.
requestCount=$(( oldCount + 1 ))
if [[ -z "$requestCount" ]]; then
    requestCount=1
fi
# First, get the names of where the file should go.
dbFileDir="$dbDir/ByCount/${requestCount:?}/$hashSubdir"
dbFilePath="$dbFileDir/$hashFull"
mkdir -p "$dbFileDir"
mv "$dbFileOldPath" "$dbFilePath"
rm "$dbLinkPath"
ln -s "$dbFilePath" "$dbLinkPath"

# Return the data to the caller
cat "$dbLinkPath"

finish
scache_gc "$dbDir" "f"
